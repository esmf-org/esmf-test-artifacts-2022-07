build time -- 2022-07-04 00:53:01
20220704 012151.499 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.499 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.499 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.499 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.499 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.499 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.499 INFO             PET0 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.499 INFO             PET0 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.499 INFO             PET0 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.499 INFO             PET0 ESMF_COMM                   : mpi
20220704 012151.500 INFO             PET0 ESMF_MOAB                   : enabled
20220704 012151.500 INFO             PET0 ESMF_LAPACK                 : enabled
20220704 012151.500 INFO             PET0 ESMF_NETCDF                 : enabled
20220704 012151.500 INFO             PET0 ESMF_PNETCDF                : disabled
20220704 012151.500 INFO             PET0 ESMF_PIO                    : enabled
20220704 012151.500 INFO             PET0 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET0 esmfComm=mpi
20220704 012151.504 INFO             PET0 isPthreadsEnabled=1
20220704 012151.504 INFO             PET0 isOpenMPEnabled=1
20220704 012151.504 INFO             PET0 isOpenACCEnabled=0
20220704 012151.504 INFO             PET0 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET0 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET0 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET0 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET0 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET0 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET0 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET0 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET0 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.504 INFO             PET0 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET0 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET0 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.505 INFO             PET0 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET0 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET0 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET0 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET0 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET0 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET0 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET0 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET0 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET0 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET0 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET0 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET0 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET0 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET0 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET0 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET0 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET0 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.505 INFO             PET0 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.505 INFO             PET0 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET0 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET0 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.505 INFO             PET0 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.505 INFO             PET0 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.505 INFO             PET0 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.505 INFO             PET0 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.505 INFO             PET0 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.505 INFO             PET0 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET0 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.505 INFO             PET0 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET0 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET0 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET0 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET0 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET0 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET0 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET0 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET0 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET0 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET0 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET0 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET0 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET0 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET0 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET0 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET0 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET0 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET0 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.506 INFO             PET0 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.506 INFO             PET0 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.506 INFO             PET0 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.506 INFO             PET0 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.506 INFO             PET0 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.506 INFO             PET0 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.506 INFO             PET0 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.506 INFO             PET0 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.506 INFO             PET0 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.506 INFO             PET0 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.506 INFO             PET0 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.506 INFO             PET0 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.506 INFO             PET0 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.506 INFO             PET0 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.506 INFO             PET0 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.506 INFO             PET0 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.506 INFO             PET0 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.506 INFO             PET0 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.506 INFO             PET0 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.506 INFO             PET0 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.506 INFO             PET0 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.506 INFO             PET0 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET0 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET0 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET0 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET0 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET0 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET0 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET0 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET0 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET0 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET0 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.507 INFO             PET0 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.507 INFO             PET0 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.507 INFO             PET0 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.507 INFO             PET0 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.507 INFO             PET0 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.507 INFO             PET0 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.507 INFO             PET0 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.507 INFO             PET0 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.507 INFO             PET0 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.507 INFO             PET0 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.507 INFO             PET0 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.507 INFO             PET0 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.507 INFO             PET0 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET0 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET0 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.507 INFO             PET0 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.507 INFO             PET0 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.507 INFO             PET0 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.507 INFO             PET0 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.507 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220704 012151.507 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220704 012151.507 INFO             PET0 main: vm located at: 0x8166f0
20220704 012151.507 INFO             PET0 main: petCount=6 localPet=0 mypthid=140736414213184 currentSsiPe=0
20220704 012151.507 INFO             PET0 main: Current system level affinity pinning for local PET:
20220704 012151.507 INFO             PET0 main:  SSIPE=0
20220704 012151.507 INFO             PET0 main:  SSIPE=36
20220704 012151.507 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.507 INFO             PET0 main: ssiCount=1 localSsi=0
20220704 012151.507 INFO             PET0 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET0 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET0 Executing 'userm1_register'
20220704 012151.509 INFO             PET0 Executing 'userm2_setvm'
20220704 012151.510 INFO             PET0 Executing 'userm2_register'
20220704 012151.512 INFO             PET0 Entering 'user1_run'
20220704 012151.512 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220704 012151.512 INFO             PET0 model1: vm located at: 0x894ee0
20220704 012151.512 INFO             PET0 model1: petCount=6 localPet=0 mypthid=140736414213184 currentSsiPe=0
20220704 012151.512 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220704 012151.512 INFO             PET0 model1:  SSIPE=0
20220704 012151.512 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET0 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET0 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012152.623 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012153.645 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012154.664 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012155.683 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012156.703 INFO             PET0 Exiting 'user1_run'
20220704 012156.709 INFO             PET0 Entering 'user2_run'
20220704 012156.709 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220704 012156.709 INFO             PET0 model2: vm located at: 0x898800
20220704 012156.709 INFO             PET0 model2: petCount=2 localPet=0 mypthid=140736414213184 currentSsiPe=0
20220704 012156.709 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220704 012156.709 INFO             PET0 model2:  SSIPE=0
20220704 012156.709 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220704 012156.709 INFO             PET0 model2: ssiCount=1 localSsi=0
20220704 012156.709 INFO             PET0 model2: mpionly=1 threadsflag=0
20220704 012156.709 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012156.709 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220704 012156.709 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220704 012156.709 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220704 012156.709 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220704 012156.709 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220704 012156.709 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220704 012156.709 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220704 012156.709 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220704 012156.709 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220704 012156.710 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012156.710 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012156.710 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012157.692 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012157.692 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012157.692 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012158.627 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012158.627 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012158.627 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012159.560 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012159.560 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012159.560 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012200.492 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012200.492 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012200.492 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012201.424 INFO             PET0  user2_run: All data correct.
20220704 012201.425 INFO             PET0 Exiting 'user2_run'
20220704 012201.425 INFO             PET0 Entering 'user1_run'
20220704 012201.425 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET0 model1: vm located at: 0x894ee0
20220704 012201.425 INFO             PET0 model1: petCount=6 localPet=0 mypthid=140736414213184 currentSsiPe=0
20220704 012201.425 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET0 model1:  SSIPE=0
20220704 012201.425 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET0 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET0 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012202.454 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012203.480 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012204.499 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012205.518 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220704 012206.536 INFO             PET0 Exiting 'user1_run'
20220704 012206.537 INFO             PET0 Entering 'user2_run'
20220704 012206.537 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220704 012206.537 INFO             PET0 model2: vm located at: 0x898800
20220704 012206.537 INFO             PET0 model2: petCount=2 localPet=0 mypthid=140736414213184 currentSsiPe=0
20220704 012206.537 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220704 012206.537 INFO             PET0 model2:  SSIPE=0
20220704 012206.537 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220704 012206.537 INFO             PET0 model2: ssiCount=1 localSsi=0
20220704 012206.537 INFO             PET0 model2: mpionly=1 threadsflag=0
20220704 012206.537 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012206.537 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220704 012206.537 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220704 012206.537 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220704 012206.537 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220704 012206.537 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220704 012206.537 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220704 012206.537 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220704 012206.537 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220704 012206.537 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220704 012206.537 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012206.537 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012206.537 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012207.469 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012207.469 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012207.469 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012208.401 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012208.401 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012208.401 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012209.332 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012209.332 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012209.332 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012210.262 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220704 012210.262 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012210.262 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012211.191 INFO             PET0  user2_run: All data correct.
20220704 012211.191 INFO             PET0 Exiting 'user2_run'
20220704 012211.192 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220704 012211.192 INFO             PET0  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.192 INFO             PET0 Finalizing ESMF
20220704 012151.490 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.490 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.490 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.490 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.490 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.490 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.491 INFO             PET1 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.491 INFO             PET1 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.491 INFO             PET1 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.491 INFO             PET1 ESMF_COMM                   : mpi
20220704 012151.491 INFO             PET1 ESMF_MOAB                   : enabled
20220704 012151.491 INFO             PET1 ESMF_LAPACK                 : enabled
20220704 012151.491 INFO             PET1 ESMF_NETCDF                 : enabled
20220704 012151.491 INFO             PET1 ESMF_PNETCDF                : disabled
20220704 012151.491 INFO             PET1 ESMF_PIO                    : enabled
20220704 012151.491 INFO             PET1 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET1 esmfComm=mpi
20220704 012151.504 INFO             PET1 isPthreadsEnabled=1
20220704 012151.504 INFO             PET1 isOpenMPEnabled=1
20220704 012151.504 INFO             PET1 isOpenACCEnabled=0
20220704 012151.504 INFO             PET1 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET1 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET1 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET1 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET1 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET1 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET1 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET1 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET1 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.505 INFO             PET1 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.505 INFO             PET1 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.505 INFO             PET1 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.505 INFO             PET1 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET1 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET1 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET1 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET1 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET1 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET1 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET1 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET1 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET1 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET1 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET1 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET1 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET1 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET1 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET1 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET1 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET1 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.506 INFO             PET1 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.506 INFO             PET1 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.506 INFO             PET1 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.506 INFO             PET1 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.506 INFO             PET1 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.506 INFO             PET1 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.506 INFO             PET1 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.506 INFO             PET1 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.506 INFO             PET1 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.506 INFO             PET1 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.506 INFO             PET1 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.506 INFO             PET1 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET1 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET1 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET1 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET1 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET1 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET1 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET1 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET1 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET1 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET1 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET1 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET1 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET1 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET1 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET1 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET1 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET1 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET1 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.507 INFO             PET1 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.507 INFO             PET1 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.507 INFO             PET1 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.507 INFO             PET1 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.507 INFO             PET1 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.507 INFO             PET1 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.507 INFO             PET1 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.507 INFO             PET1 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.507 INFO             PET1 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.507 INFO             PET1 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.507 INFO             PET1 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.507 INFO             PET1 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.507 INFO             PET1 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.507 INFO             PET1 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.507 INFO             PET1 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.507 INFO             PET1 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.507 INFO             PET1 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.507 INFO             PET1 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.507 INFO             PET1 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.507 INFO             PET1 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.507 INFO             PET1 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.507 INFO             PET1 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET1 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET1 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET1 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET1 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET1 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET1 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET1 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET1 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET1 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET1 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.508 INFO             PET1 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.508 INFO             PET1 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.508 INFO             PET1 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.508 INFO             PET1 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.508 INFO             PET1 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.508 INFO             PET1 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.508 INFO             PET1 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.508 INFO             PET1 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.508 INFO             PET1 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.508 INFO             PET1 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.508 INFO             PET1 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.508 INFO             PET1 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.508 INFO             PET1 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.508 INFO             PET1 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.508 INFO             PET1 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.508 INFO             PET1 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.508 INFO             PET1 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.508 INFO             PET1 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.508 INFO             PET1 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.508 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220704 012151.508 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220704 012151.508 INFO             PET1 main: vm located at: 0x816680
20220704 012151.508 INFO             PET1 main: petCount=6 localPet=1 mypthid=140736414213184 currentSsiPe=18
20220704 012151.508 INFO             PET1 main: Current system level affinity pinning for local PET:
20220704 012151.508 INFO             PET1 main:  SSIPE=18
20220704 012151.508 INFO             PET1 main:  SSIPE=54
20220704 012151.508 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.508 INFO             PET1 main: ssiCount=1 localSsi=0
20220704 012151.508 INFO             PET1 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET1 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET1 Executing 'userm1_register'
20220704 012151.509 INFO             PET1 Executing 'userm2_setvm'
20220704 012151.509 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.510 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.512 INFO             PET1 Entering 'user1_run'
20220704 012151.512 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220704 012151.513 INFO             PET1 model1: vm located at: 0x894370
20220704 012151.513 INFO             PET1 model1: petCount=6 localPet=1 mypthid=140736414213184 currentSsiPe=1
20220704 012151.513 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220704 012151.513 INFO             PET1 model1:  SSIPE=1
20220704 012151.513 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET1 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET1 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012152.624 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012153.646 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012154.662 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012155.680 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012156.700 INFO             PET1 Exiting 'user1_run'
20220704 012201.425 INFO             PET1 Entering 'user1_run'
20220704 012201.425 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET1 model1: vm located at: 0x894370
20220704 012201.425 INFO             PET1 model1: petCount=6 localPet=1 mypthid=140736414213184 currentSsiPe=1
20220704 012201.425 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET1 model1:  SSIPE=1
20220704 012201.425 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET1 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET1 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012202.452 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012203.478 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012204.497 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012205.517 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012206.536 INFO             PET1 Exiting 'user1_run'
20220704 012211.192 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220704 012211.192 INFO             PET1  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.192 INFO             PET1 Finalizing ESMF
20220704 012151.497 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.497 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.497 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.497 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.497 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.497 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.497 INFO             PET2 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.497 INFO             PET2 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.497 INFO             PET2 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.497 INFO             PET2 ESMF_COMM                   : mpi
20220704 012151.498 INFO             PET2 ESMF_MOAB                   : enabled
20220704 012151.498 INFO             PET2 ESMF_LAPACK                 : enabled
20220704 012151.498 INFO             PET2 ESMF_NETCDF                 : enabled
20220704 012151.498 INFO             PET2 ESMF_PNETCDF                : disabled
20220704 012151.498 INFO             PET2 ESMF_PIO                    : enabled
20220704 012151.498 INFO             PET2 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET2 esmfComm=mpi
20220704 012151.504 INFO             PET2 isPthreadsEnabled=1
20220704 012151.504 INFO             PET2 isOpenMPEnabled=1
20220704 012151.504 INFO             PET2 isOpenACCEnabled=0
20220704 012151.504 INFO             PET2 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET2 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET2 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET2 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET2 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET2 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET2 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET2 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET2 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.504 INFO             PET2 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET2 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET2 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.505 INFO             PET2 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET2 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET2 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET2 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET2 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET2 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET2 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET2 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET2 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET2 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET2 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET2 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET2 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET2 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET2 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET2 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET2 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET2 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.505 INFO             PET2 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.505 INFO             PET2 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET2 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET2 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.505 INFO             PET2 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.505 INFO             PET2 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.505 INFO             PET2 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.505 INFO             PET2 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.505 INFO             PET2 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.505 INFO             PET2 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET2 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.506 INFO             PET2 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET2 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET2 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET2 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET2 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET2 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET2 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET2 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET2 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET2 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET2 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET2 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET2 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET2 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET2 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET2 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET2 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET2 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET2 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.506 INFO             PET2 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.506 INFO             PET2 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.506 INFO             PET2 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.506 INFO             PET2 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.506 INFO             PET2 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.506 INFO             PET2 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.506 INFO             PET2 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.506 INFO             PET2 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.506 INFO             PET2 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.506 INFO             PET2 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.506 INFO             PET2 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.506 INFO             PET2 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.506 INFO             PET2 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.506 INFO             PET2 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.506 INFO             PET2 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.506 INFO             PET2 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.506 INFO             PET2 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.506 INFO             PET2 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.506 INFO             PET2 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.506 INFO             PET2 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.506 INFO             PET2 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.506 INFO             PET2 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET2 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET2 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET2 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET2 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET2 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET2 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET2 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET2 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET2 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET2 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.507 INFO             PET2 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.507 INFO             PET2 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.507 INFO             PET2 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.507 INFO             PET2 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.507 INFO             PET2 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.507 INFO             PET2 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.507 INFO             PET2 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.507 INFO             PET2 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.507 INFO             PET2 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.507 INFO             PET2 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.507 INFO             PET2 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.507 INFO             PET2 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.507 INFO             PET2 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET2 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET2 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.507 INFO             PET2 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.507 INFO             PET2 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.507 INFO             PET2 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.507 INFO             PET2 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.507 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220704 012151.507 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220704 012151.507 INFO             PET2 main: vm located at: 0x816680
20220704 012151.507 INFO             PET2 main: petCount=6 localPet=2 mypthid=140736414213184 currentSsiPe=1
20220704 012151.507 INFO             PET2 main: Current system level affinity pinning for local PET:
20220704 012151.507 INFO             PET2 main:  SSIPE=1
20220704 012151.507 INFO             PET2 main:  SSIPE=37
20220704 012151.507 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.507 INFO             PET2 main: ssiCount=1 localSsi=0
20220704 012151.507 INFO             PET2 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET2 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET2 Executing 'userm1_register'
20220704 012151.509 INFO             PET2 Executing 'userm2_setvm'
20220704 012151.509 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.510 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.512 INFO             PET2 Entering 'user1_run'
20220704 012151.512 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220704 012151.512 INFO             PET2 model1: vm located at: 0x894370
20220704 012151.512 INFO             PET2 model1: petCount=6 localPet=2 mypthid=140736414213184 currentSsiPe=2
20220704 012151.512 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220704 012151.513 INFO             PET2 model1:  SSIPE=2
20220704 012151.513 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET2 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET2 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012152.622 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012153.644 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012154.662 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012155.682 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012156.701 INFO             PET2 Exiting 'user1_run'
20220704 012201.425 INFO             PET2 Entering 'user1_run'
20220704 012201.425 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET2 model1: vm located at: 0x894370
20220704 012201.425 INFO             PET2 model1: petCount=6 localPet=2 mypthid=140736414213184 currentSsiPe=2
20220704 012201.425 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET2 model1:  SSIPE=2
20220704 012201.425 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET2 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET2 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012202.451 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012203.477 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012204.496 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012205.515 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220704 012206.534 INFO             PET2 Exiting 'user1_run'
20220704 012211.192 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220704 012211.192 INFO             PET2  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.192 INFO             PET2 Finalizing ESMF
20220704 012151.503 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.503 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.503 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.503 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.503 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.503 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.503 INFO             PET3 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.503 INFO             PET3 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.503 INFO             PET3 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.503 INFO             PET3 ESMF_COMM                   : mpi
20220704 012151.504 INFO             PET3 ESMF_MOAB                   : enabled
20220704 012151.504 INFO             PET3 ESMF_LAPACK                 : enabled
20220704 012151.504 INFO             PET3 ESMF_NETCDF                 : enabled
20220704 012151.504 INFO             PET3 ESMF_PNETCDF                : disabled
20220704 012151.504 INFO             PET3 ESMF_PIO                    : enabled
20220704 012151.504 INFO             PET3 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET3 esmfComm=mpi
20220704 012151.504 INFO             PET3 isPthreadsEnabled=1
20220704 012151.504 INFO             PET3 isOpenMPEnabled=1
20220704 012151.504 INFO             PET3 isOpenACCEnabled=0
20220704 012151.504 INFO             PET3 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET3 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET3 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET3 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET3 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET3 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET3 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET3 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET3 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.504 INFO             PET3 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET3 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.505 INFO             PET3 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.505 INFO             PET3 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET3 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET3 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET3 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET3 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET3 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET3 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET3 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET3 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET3 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET3 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET3 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET3 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET3 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET3 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET3 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET3 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET3 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.505 INFO             PET3 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.505 INFO             PET3 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET3 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET3 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.505 INFO             PET3 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.505 INFO             PET3 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.505 INFO             PET3 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.505 INFO             PET3 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.505 INFO             PET3 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.506 INFO             PET3 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.506 INFO             PET3 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.506 INFO             PET3 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET3 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET3 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET3 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET3 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET3 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET3 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET3 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET3 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET3 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET3 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET3 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET3 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET3 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET3 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET3 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET3 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET3 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET3 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.506 INFO             PET3 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.506 INFO             PET3 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.506 INFO             PET3 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.506 INFO             PET3 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.506 INFO             PET3 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.506 INFO             PET3 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.506 INFO             PET3 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.506 INFO             PET3 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.506 INFO             PET3 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.506 INFO             PET3 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.506 INFO             PET3 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.506 INFO             PET3 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.506 INFO             PET3 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.506 INFO             PET3 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.507 INFO             PET3 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.507 INFO             PET3 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.507 INFO             PET3 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.507 INFO             PET3 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.507 INFO             PET3 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.507 INFO             PET3 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.507 INFO             PET3 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.507 INFO             PET3 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET3 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET3 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET3 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET3 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET3 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET3 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET3 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET3 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET3 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET3 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.507 INFO             PET3 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.507 INFO             PET3 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.507 INFO             PET3 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.507 INFO             PET3 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.507 INFO             PET3 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.507 INFO             PET3 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.507 INFO             PET3 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.507 INFO             PET3 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.507 INFO             PET3 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.507 INFO             PET3 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.507 INFO             PET3 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.507 INFO             PET3 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.507 INFO             PET3 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET3 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET3 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.507 INFO             PET3 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.507 INFO             PET3 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.507 INFO             PET3 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.507 INFO             PET3 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.508 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220704 012151.508 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220704 012151.508 INFO             PET3 main: vm located at: 0x816680
20220704 012151.508 INFO             PET3 main: petCount=6 localPet=3 mypthid=140736414213184 currentSsiPe=55
20220704 012151.508 INFO             PET3 main: Current system level affinity pinning for local PET:
20220704 012151.508 INFO             PET3 main:  SSIPE=19
20220704 012151.508 INFO             PET3 main:  SSIPE=55
20220704 012151.508 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.508 INFO             PET3 main: ssiCount=1 localSsi=0
20220704 012151.508 INFO             PET3 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET3 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET3 Executing 'userm1_register'
20220704 012151.509 INFO             PET3 Executing 'userm2_setvm'
20220704 012151.510 INFO             PET3 Executing 'userm2_register'
20220704 012151.512 INFO             PET3 Entering 'user1_run'
20220704 012151.512 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220704 012151.512 INFO             PET3 model1: vm located at: 0x894370
20220704 012151.512 INFO             PET3 model1: petCount=6 localPet=3 mypthid=140736414213184 currentSsiPe=3
20220704 012151.512 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220704 012151.513 INFO             PET3 model1:  SSIPE=3
20220704 012151.513 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET3 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET3 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012152.626 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012153.648 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012154.667 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012155.687 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012156.706 INFO             PET3 Exiting 'user1_run'
20220704 012156.709 INFO             PET3 Entering 'user2_run'
20220704 012156.709 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220704 012156.709 INFO             PET3 model2: vm located at: 0x897820
20220704 012156.709 INFO             PET3 model2: petCount=2 localPet=1 mypthid=140736414213184 currentSsiPe=3
20220704 012156.709 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220704 012156.709 INFO             PET3 model2:  SSIPE=3
20220704 012156.709 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220704 012156.709 INFO             PET3 model2: ssiCount=1 localSsi=0
20220704 012156.709 INFO             PET3 model2: mpionly=1 threadsflag=0
20220704 012156.709 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012156.709 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220704 012156.709 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220704 012156.709 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220704 012156.709 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220704 012156.709 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220704 012156.709 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220704 012156.709 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220704 012156.709 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220704 012156.709 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220704 012156.710 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012156.710 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012156.710 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012157.691 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012157.691 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012157.691 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012158.626 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012158.626 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012158.626 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012159.560 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012159.560 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012159.560 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012200.492 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012200.492 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012200.492 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012201.424 INFO             PET3  user2_run: All data correct.
20220704 012201.425 INFO             PET3 Exiting 'user2_run'
20220704 012201.425 INFO             PET3 Entering 'user1_run'
20220704 012201.425 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET3 model1: vm located at: 0x894370
20220704 012201.425 INFO             PET3 model1: petCount=6 localPet=3 mypthid=140736414213184 currentSsiPe=3
20220704 012201.425 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET3 model1:  SSIPE=3
20220704 012201.425 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET3 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET3 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012202.452 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012203.478 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012204.498 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012205.517 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012206.536 INFO             PET3 Exiting 'user1_run'
20220704 012206.536 INFO             PET3 Entering 'user2_run'
20220704 012206.536 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220704 012206.536 INFO             PET3 model2: vm located at: 0x897820
20220704 012206.537 INFO             PET3 model2: petCount=2 localPet=1 mypthid=140736414213184 currentSsiPe=3
20220704 012206.537 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220704 012206.537 INFO             PET3 model2:  SSIPE=3
20220704 012206.537 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220704 012206.537 INFO             PET3 model2: ssiCount=1 localSsi=0
20220704 012206.537 INFO             PET3 model2: mpionly=1 threadsflag=0
20220704 012206.537 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012206.537 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220704 012206.537 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220704 012206.537 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220704 012206.537 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220704 012206.537 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220704 012206.537 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220704 012206.537 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220704 012206.537 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220704 012206.537 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220704 012206.537 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012206.537 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012206.537 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012207.469 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012207.469 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012207.469 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012208.401 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012208.401 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012208.401 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012209.332 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012209.332 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012209.332 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012210.261 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220704 012210.262 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012210.262 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220704 012211.191 INFO             PET3  user2_run: All data correct.
20220704 012211.191 INFO             PET3 Exiting 'user2_run'
20220704 012211.191 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220704 012211.191 INFO             PET3  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.191 INFO             PET3 Finalizing ESMF
20220704 012151.491 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.491 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.491 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.491 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.491 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.491 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.491 INFO             PET4 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.491 INFO             PET4 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.491 INFO             PET4 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.491 INFO             PET4 ESMF_COMM                   : mpi
20220704 012151.492 INFO             PET4 ESMF_MOAB                   : enabled
20220704 012151.492 INFO             PET4 ESMF_LAPACK                 : enabled
20220704 012151.492 INFO             PET4 ESMF_NETCDF                 : enabled
20220704 012151.492 INFO             PET4 ESMF_PNETCDF                : disabled
20220704 012151.492 INFO             PET4 ESMF_PIO                    : enabled
20220704 012151.492 INFO             PET4 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET4 esmfComm=mpi
20220704 012151.504 INFO             PET4 isPthreadsEnabled=1
20220704 012151.504 INFO             PET4 isOpenMPEnabled=1
20220704 012151.504 INFO             PET4 isOpenACCEnabled=0
20220704 012151.504 INFO             PET4 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET4 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET4 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET4 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET4 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET4 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET4 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET4 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET4 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.504 INFO             PET4 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET4 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET4 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.504 INFO             PET4 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET4 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET4 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET4 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET4 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET4 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET4 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET4 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET4 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET4 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET4 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET4 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET4 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET4 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET4 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET4 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET4 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET4 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.505 INFO             PET4 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.505 INFO             PET4 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET4 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET4 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.505 INFO             PET4 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.505 INFO             PET4 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.506 INFO             PET4 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.506 INFO             PET4 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.506 INFO             PET4 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.506 INFO             PET4 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.506 INFO             PET4 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.506 INFO             PET4 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET4 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET4 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET4 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET4 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET4 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET4 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET4 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET4 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET4 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET4 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET4 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET4 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET4 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET4 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET4 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET4 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET4 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET4 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.506 INFO             PET4 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.506 INFO             PET4 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.506 INFO             PET4 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.506 INFO             PET4 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.506 INFO             PET4 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.506 INFO             PET4 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.506 INFO             PET4 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.506 INFO             PET4 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.506 INFO             PET4 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.506 INFO             PET4 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.506 INFO             PET4 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.506 INFO             PET4 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.506 INFO             PET4 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.506 INFO             PET4 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.507 INFO             PET4 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.507 INFO             PET4 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.507 INFO             PET4 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.507 INFO             PET4 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.507 INFO             PET4 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.507 INFO             PET4 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.507 INFO             PET4 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.507 INFO             PET4 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET4 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET4 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET4 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET4 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET4 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET4 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET4 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET4 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET4 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET4 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.507 INFO             PET4 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.507 INFO             PET4 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.507 INFO             PET4 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.507 INFO             PET4 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.507 INFO             PET4 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.507 INFO             PET4 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.507 INFO             PET4 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.507 INFO             PET4 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.507 INFO             PET4 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.507 INFO             PET4 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.507 INFO             PET4 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.507 INFO             PET4 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.507 INFO             PET4 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET4 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET4 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.507 INFO             PET4 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.507 INFO             PET4 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.507 INFO             PET4 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.507 INFO             PET4 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.507 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220704 012151.507 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220704 012151.507 INFO             PET4 main: vm located at: 0x816680
20220704 012151.507 INFO             PET4 main: petCount=6 localPet=4 mypthid=140736414213184 currentSsiPe=2
20220704 012151.508 INFO             PET4 main: Current system level affinity pinning for local PET:
20220704 012151.508 INFO             PET4 main:  SSIPE=2
20220704 012151.508 INFO             PET4 main:  SSIPE=38
20220704 012151.508 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.508 INFO             PET4 main: ssiCount=1 localSsi=0
20220704 012151.508 INFO             PET4 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET4 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET4 Executing 'userm1_register'
20220704 012151.509 INFO             PET4 Executing 'userm2_setvm'
20220704 012151.509 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.510 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.512 INFO             PET4 Entering 'user1_run'
20220704 012151.512 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220704 012151.512 INFO             PET4 model1: vm located at: 0x8943b0
20220704 012151.512 INFO             PET4 model1: petCount=6 localPet=4 mypthid=140736414213184 currentSsiPe=4
20220704 012151.513 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220704 012151.513 INFO             PET4 model1:  SSIPE=4
20220704 012151.513 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET4 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET4 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012152.625 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012153.646 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012154.664 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012155.683 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012156.701 INFO             PET4 Exiting 'user1_run'
20220704 012201.425 INFO             PET4 Entering 'user1_run'
20220704 012201.425 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET4 model1: vm located at: 0x8943b0
20220704 012201.425 INFO             PET4 model1: petCount=6 localPet=4 mypthid=140736414213184 currentSsiPe=4
20220704 012201.425 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET4 model1:  SSIPE=4
20220704 012201.425 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET4 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET4 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012202.453 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012203.479 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012204.497 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012205.515 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220704 012206.534 INFO             PET4 Exiting 'user1_run'
20220704 012211.191 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220704 012211.192 INFO             PET4  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.192 INFO             PET4 Finalizing ESMF
20220704 012151.496 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.496 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220704 012151.496 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220704 012151.496 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220704 012151.496 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220704 012151.496 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220704 012151.496 INFO             PET5 Running with ESMF Version   : v8.4.0b02-7-g9328c9b05f
20220704 012151.496 INFO             PET5 ESMF library build date/time: "Jul  4 2022" "00:50:51"
20220704 012151.496 INFO             PET5 ESMF library build location : /lustre/f2/dev/ncep/Mark.Potts/intel_2019.5_mpi_g_develop
20220704 012151.496 INFO             PET5 ESMF_COMM                   : mpi
20220704 012151.497 INFO             PET5 ESMF_MOAB                   : enabled
20220704 012151.497 INFO             PET5 ESMF_LAPACK                 : enabled
20220704 012151.497 INFO             PET5 ESMF_NETCDF                 : enabled
20220704 012151.497 INFO             PET5 ESMF_PNETCDF                : disabled
20220704 012151.497 INFO             PET5 ESMF_PIO                    : enabled
20220704 012151.497 INFO             PET5 ESMF_YAMLCPP                : enabled
20220704 012151.504 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220704 012151.504 INFO             PET5 esmfComm=mpi
20220704 012151.504 INFO             PET5 isPthreadsEnabled=1
20220704 012151.504 INFO             PET5 isOpenMPEnabled=1
20220704 012151.504 INFO             PET5 isOpenACCEnabled=0
20220704 012151.504 INFO             PET5 isSsiSharedMemoryEnabled=1
20220704 012151.504 INFO             PET5 ssiCount=1 peCount=6
20220704 012151.504 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220704 012151.504 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220704 012151.504 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220704 012151.504 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220704 012151.504 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220704 012151.504 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220704 012151.504 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220704 012151.504 INFO             PET5 index=   0                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220704 012151.504 INFO             PET5 index=   1                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220704 012151.504 INFO             PET5 index=   2                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220704 012151.504 INFO             PET5 index=   3                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220704 012151.504 INFO             PET5 index=   4                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220704 012151.504 INFO             PET5 index=   5                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220704 012151.504 INFO             PET5 index=   6                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220704 012151.504 INFO             PET5 index=   7                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.504 INFO             PET5 index=   8                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220704 012151.505 INFO             PET5 index=   9                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220704 012151.505 INFO             PET5 index=  10                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220704 012151.505 INFO             PET5 index=  11                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220704 012151.505 INFO             PET5 index=  12                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220704 012151.505 INFO             PET5 index=  13                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220704 012151.505 INFO             PET5 index=  14                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220704 012151.505 INFO             PET5 index=  15                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220704 012151.505 INFO             PET5 index=  16                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220704 012151.505 INFO             PET5 index=  17                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220704 012151.505 INFO             PET5 index=  18                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized binomial tree scatterv algorithm is used instead of the default ANL scatterv algorithm. The optimized algorithm is better-suited for small messages, especially at large scale. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220704 012151.505 INFO             PET5 index=  19                             MPIR_CVAR_DMAPP_A2A_SYMBUF_SIZE : (Gemini systems only) Specifies the size (in bytes) of the symmetric heap that will be used for the Gemini DMAPP- optimized Alltoall algorithm. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 256 * number_of_ranks, or 32MB, whichever is smaller
20220704 012151.505 INFO             PET5 index=  20                               MPIR_CVAR_DMAPP_A2A_SHORT_MSG : (Gemini systems only) Specifies the cutoff size (in bytes) at or below which the Gemini DMAPP-optimized Alltoall algorithm will be used. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems. Default: 4096 bytes
20220704 012151.505 INFO             PET5 index=  21                                MPIR_CVAR_DMAPP_A2A_USE_PUTS : (Gemini systems only) If set, the Gemini DMAPP-optimized Alltoall algorithm will use PUTs instead of GETs. Generally, as long as huge pages are used, GETs perform better. If huge pages are not used, it is advisable to select PUTs. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Alltoall optimization feature and supported only on Gemini (Cray XE and Cray XK) systems.
20220704 012151.505 INFO             PET5 index=  22                                    MPIR_CVAR_USE_DMAPP_COLL : If set, the MPICH library will attempt to use the highly optimized GHAL-based DMAPP collective algorithms, if available. On Gemini systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Barrier, MPI_Alltoall, and MPI_Iallreduce. On Aries systems, the supported DMAPP collectives are MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast, plus access to the hardware collective engine for MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to 1. To enable a specific set of DMAPP optimized collective algorithms, set MPICH_USE_DMAPP_COLL to a comma-separated list of the desired collective names. For example, to enable only the MPI_Allreduce DMAPP optimized collective, set MPICH_USE_DMAPP_COLL=mpi_allreduce. Names are not case- sensitive. Any unsupported name is flagged with a warning message and ignored. There are several restrictions that must be met before these DMAPP algorithms can be used.  See the intro
20220704 012151.505 INFO             PET5 index=  23                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET5 index=  24                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220704 012151.505 INFO             PET5 index=  25                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.505 INFO             PET5 index=  26                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: On Aries systems, when using the default uGNI Alltoall algorithm or selecting the DMAPP Alltoall algorithm, the defaults are: if communicator size <=256, 64 bytes if communicator size >256 and <=1024, 32 bytes if communicator size >1024 and <=4096, 16 bytes if communicator size >4096, 8 bytes On Gemini systems, or if using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220704 012151.505 INFO             PET5 index=  27                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. On Gemini systems, for small messages, consider increasing the throttle to 2 or 3 to improve performance. On Aries systems, this variable has no effect when using the default uGNI-optimized MPI_Alltoallv algorithm. Use the MPICH_GNI_A2A_* environment variables instead. If the uGNI- optimized version of MPI_Alltoallv is disabled, then this variable works as documented. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 1 (Gemini systems), 8 (Aries systems)
20220704 012151.505 INFO             PET5 index=  28                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220704 012151.505 INFO             PET5 index=  29                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET5 index=  30                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220704 012151.505 INFO             PET5 index=  31                                MPIR_CVAR_COLL_BAL_INJECTION : Used to adjust the automatic balanced injection feature for optimizing MPI_Alltoall and MPI_Alltoallv communication. Note:  This environment variable applies to Cray systems with Gemini interconnect (Cray XE, Cray XK) only. It has no effect on Cray systems that use the Aries interconnect. By default, MPI automatically selects appropriate balanced injection settings, based in part on the number of nodes in the Alltoall/v communicator. To disable balanced injection in MPI, set this variable to 0. To override MPI's default balanced injection settings and instead use a specific balanced injection value, set this variable to the desired balanced injection value in the range of 1 to 100. Default: unset (auto balanced injection enabled)
20220704 012151.505 INFO             PET5 index=  32                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220704 012151.505 INFO             PET5 index=  33                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220704 012151.505 INFO             PET5 index=  34                                  MPIR_CVAR_DMAPP_COLL_RADIX : Sets the size of the radix for the GHAL-based DMAPP MPI_Allreduce, MPI_Iallreduce, and MPI_Barrier collective algorithms. The supported sizes are 4, 8, 16, 32, or 64. This environment variable is applicable only if MPICH_USE_DMAPP_COLL is set. Default: 64
20220704 012151.505 INFO             PET5 index=  35                                       MPIR_CVAR_DMAPP_HW_CE : (Aries systems only) Controls whether the Aries hardware collective engine (CE) is used for MPI_Barrier, MPI_Allreduce, and MPI_Iallreduce calls. This environment variable applies only if MPICH_USE_DMAPP_COLL is set to enable the DMAPP MPI_Allreduce, MPI_Iallreduce, and/or MPI_Barrier optimization features. If MPICH_DMAPP_HW_CE is set to enabled or 1, the CE is used for qualifying calls. If set to disabled or 0, the CE is not used. The CE supports a limited subset of sizes and operations for MPI_Allreduce and MPI_Iallreduce. If the CE cannot be used, MPICH falls back to the DMAPP software-optimized versions. If those DMAPP versions cannot be used, the MPICH versions are used. This environment variable is supported only on Aries (Cray XC series) systems. Default: If MPICH_USE_DMAPP_COLL is set, MPICH_DMAPP_HW_CE defaults to enabled, provided DMAPP 6.0 or later is used. If a previous version of DMAPP is detected, the environment variable defaults to disabled.
20220704 012151.505 INFO             PET5 index=  36                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at which and below which the architecture-specific optimized MPI_Gatherv algorithm is used instead of the default MPICH MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes. The optimized algorithm is better suited for scaling to high process counts, especially for small- to medium-sized messages. Default: 16384 bytes
20220704 012151.506 INFO             PET5 index=  37                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220704 012151.506 INFO             PET5 index=  38                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220704 012151.506 INFO             PET5 index=  39                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. On Aries systems, the shared-memory based optimization for MPI_Allreduce can also be used in conjunction with the highly optimized DMAPP MPI_Allreduce algorithm. See MPICH_USE_DMAPP_COLL for additional information. Default: set
20220704 012151.506 INFO             PET5 index=  40                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220704 012151.506 INFO             PET5 index=  41                                   MPIR_CVAR_DMAPP_A2A_ARIES : (Aries systems only) If set, requests use of the DMAPP-optimized MPI_Alltoall algorithm to be used.  By default, the uGNI MPI_Alltoall algorithm is used. Use of the DMAPP MPI_Alltoall collective on Aries requires a contiguous data type and begins when the all-to-all message size is  greater than the value set using the MPICH_ALLTOALL_SHORT_MSG  environment variable. Using hugepages is strongly recommended for  best performance. This DMAPP algorithm was originally selectable via setting  MPICH_USE_DMAPP_COLL to 1 or MPI_Alltoall. However that option has since been deprecated on Aries. Default: not enabled
20220704 012151.506 INFO             PET5 index=  42                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDSCAT_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220704 012151.506 INFO             PET5 index=  43                              MPIR_CVAR_REDSCAT_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the  pairwise exchange algorithm, provided the op is commutative.  The  pairwise exchange algorithm is not well suited for scaling to high  process counts, so for larger communicators, a recursive halving  algorithm is used instead.
20220704 012151.506 INFO             PET5 index=  44                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220704 012151.506 INFO             PET5 index=  45                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_RDMA_ENABLED_CUDA is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_RDMA_ENABLED_CUDA is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8. Pipelining is always used for rendezvous path messages on Gemini networks. On Aries networks, it is used only for sufficiently large messages, where the threshold for pipelining GPU-to-GPU messages depends on the family and model number of the host CPU. Default: not set
20220704 012151.506 INFO             PET5 index=  46                                     MPIR_CVAR_NO_GPU_DIRECT : If true, GPUDirect is not used for GPU-to-GPU transfers. This is mainly a debugging method used to determine if stale data is being sent across the network due to an MPI communication function being called before a data transfer to the send buffer has completed. Default: 0
20220704 012151.506 INFO             PET5 index=  47                                 MPIR_CVAR_RDMA_ENABLED_CUDA : If set, allows the MPI application to pass GPU pointers directly to point-to-point and collective communication functions. Note that the GPU-to-GPU feature is not yet supported for any functions introduced in the MPI-3 standard. Currently, if the send or receive buffer for a point-to- point or collective communication is on the GPU, the network transfer and the transfer between the host CPU and the GPU are pipelined to improve performance. Future implementations may use an RDMA-based approach to write/read data directly to/from the GPU, bypassing the host CPU. Default: not set
20220704 012151.506 INFO             PET5 index=  48                                      MPIR_CVAR_RMA_FALLBACK : Fallback to our two-sided RMA implementation (1), or fall back to ANL's implementation (2). A value of of 0 indicates that neither fallback implementation should be used. Default:  0
20220704 012151.506 INFO             PET5 index=  49                               MPIR_CVAR_SMP_SINGLE_COPY_OFF : If set, disables single-copy mode for the SMP device and forces all on-node messages, regardless of size, to be buffered. This overrides the MPICH_SMP_SINGLE_COPY_SIZE setting. Default: not set
20220704 012151.506 INFO             PET5 index=  50                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220704 012151.506 INFO             PET5 index=  51                   MPIR_CVAR_GNI_SUPPRESS_PROC_FILE_WARNINGS : Suppress initialization warnings when GNI is unable to open certain /proc/ configuration files. Default: not enabled
20220704 012151.506 INFO             PET5 index=  52                             MPIR_CVAR_GNI_BTE_MULTI_CHANNEL : Controls use of multiple BTE channels. MPICH normally tries to use multiple BTE channels for maximum efficiency, but there may be cases in which it is preferable to use only one virtual channel. To do so, set this environment variable to disabled. Default: enabled
20220704 012151.506 INFO             PET5 index=  53                              MPIR_CVAR_GNI_DATAGRAM_TIMEOUT : Controls the maximum time in seconds that MPICH will wait before considering a connection timeout request to another rank to have timed out. A timed-out connection request is considered to be a fatal error for this MPICH release. Setting this environment variable to -1 disables this timeout feature. Default: -1 not enabled
20220704 012151.506 INFO             PET5 index=  54                                 MPIR_CVAR_GNI_DMAPP_INTEROP : On Gemini-based systems, this controls interoperability between MPICH and the SHMEM, CCE UPC, and CCE Coarray Fortran one-sided program models. If set to enabled, interoperability is enabled; if set to disabled, interoperability is disabled, which may lead to a drop in application performance and possibly application hangs. Default: enabled for Gemini-based systems. disabled for Aries-based systems.
20220704 012151.506 INFO             PET5 index=  55                                  MPIR_CVAR_GNI_DYNAMIC_CONN : By default, connections are set up on demand. This allows for optimal performance while minimizing memory requirements. If set to enabled, dynamic connections are enabled; if set to disabled, MPICH establishes all connections at job startup, which may require significant amounts of memory. Default: enabled
20220704 012151.506 INFO             PET5 index=  56                                   MPIR_CVAR_GNI_FMA_SHARING : Controls whether MPI uses dedicated or shared FMA descriptors. If set to enabled, shared FMA descriptors are used; if set to disabled, dedicated FMA descriptors are used. Default: Enabled for Aries systems running CLE 5.1-UP00 or later with Xeon processors. For Aries systems with KNL processors the default is disabled, unless FMA sharing is required based on user-specified job resources.  Disabled for Gemini systems and  Aries systems running earlier versions of CLE.
20220704 012151.506 INFO             PET5 index=  57                                     MPIR_CVAR_GNI_FORK_MODE : This environment variable controls the behavior of registered memory segments when a process invokes a fork or related system call. There are three options: NOCOPY    In the child process, unmap all pages of each registered memory region subject to copy-on-write semantics. This option consumes the least memory and takes the least time at fork time, but is more likely to cause the child process to segfault if it attempts to access one of the unmapped pages. FULLCOPY  In the child process, make new copies of all pages in registered memory regions subject to copy-on- write semantics. This option takes the most time and memory, but may be required for some applications in which the child process accesses pages in registered regions. PARTCOPY  In the child process, make new copies of the first and last page of each registered memory region subject to copy-on-write semantics and unmap any intervening pages. As a compromise between zero and full copy, this option follows the observation that "false sharing" may occ
20220704 012151.506 INFO             PET5 index=  58                                 MPIR_CVAR_GNI_HUGEPAGE_SIZE : (Aries systems only) Specifies the hugepage size in bytes that will be used for the GNI internal mailbox memory. The default size is 2MB. Jobs that scale to high process counts and have a high connectivity pattern may benefit from using a larger hugepage size for this memory, as this can reduce the number of Aries PTE misses. If setting MPICH_GNI_HUGEPAGE_SIZE to a larger value, you may also want to increase the MPICH_GNI_MBOXES_PER_BLOCK value. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, and 256M, 512M, 1G and 2G.  Default: 2M
20220704 012151.506 INFO             PET5 index=  59                                  MPIR_CVAR_GNI_LMT_GET_PATH : Controls whether or not to use an RDMA GET-based protocol for certain long message transfers. If set to disabled, the RDMA GET-based protocol is not used for long message transfers. Valid settings are enabled or disabled. Default: varies
20220704 012151.506 INFO             PET5 index=  60                                      MPIR_CVAR_GNI_LMT_PATH : Controls whether or not to use zero-copy RDMA protocols for long message transfers. If set to disabled, MPICH falls back to using internal buffers for long message transfers. Setting this environment variable to disabled also effectively sets MPICH_GNI_LMT_GET_PATH to disabled. Default: enabled
20220704 012151.506 INFO             PET5 index=  61                                 MPIR_CVAR_GNI_LOCAL_CQ_SIZE : Adjusts the GNI local CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 8192
20220704 012151.506 INFO             PET5 index=  62                               MPIR_CVAR_GNI_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220704 012151.506 INFO             PET5 index=  63                            MPIR_CVAR_GNI_MAX_EAGER_MSG_SIZE : Controls the threshold for switching from eager to rendezvous protocols for internode messaging. Default: 8192 bytes
20220704 012151.506 INFO             PET5 index=  64                               MPIR_CVAR_GNI_MAX_NUM_RETRIES : Controls the maximum number of times MPICH tries to retransmit a message if a transient network error is detected. Default: 16
20220704 012151.506 INFO             PET5 index=  65                           MPIR_CVAR_GNI_MAX_VSHORT_MSG_SIZE : Used to adjust the maximum size of the message that can be sent through a message mailbox. The default varies dynamically depending on the job size and particular MPICH release and is intended to provide optimal performance for most jobs. If a job spends too much time in point-to-point communication of short messages, users may want to experiment with different values. Valid values are between 80 and 8192 bytes. Default: varies with job size
20220704 012151.506 INFO             PET5 index=  66                                MPIR_CVAR_GNI_MBOX_PLACEMENT : Controls placement of MPI internal buffers within a node. By default, buffers are placed on the memory nearest the rank (process). If this environment variable is set to nic, the buffers are placed on the memory nearest the network interface. If set to preferred, and if an application is launched using the aprun -ss option, MPICH uses the MPOL_PREFERRED memory placement policy. This Linux memory placement policy tries to allocate pages first on the preferred node, but if pages of the requested size are not available on the preferred node, it then tries to allocate pages from other nodes. Default: not set (buffers placed on memory nearest the rank)
20220704 012151.506 INFO             PET5 index=  67                              MPIR_CVAR_GNI_MBOXES_PER_BLOCK : Controls the number of MPI internal mailboxes allocated per block. This can affect the amount of memory used and the memory registration resources required by the mailboxes. This value must be a power of two.  On Aries systems with MMD sharing enabled, the default is 4096. On Gemini systems, or Aries systems with MDD sharing disabled, by default this value changes depending on the number of ranks in the job. Default: varies
20220704 012151.506 INFO             PET5 index=  68                                   MPIR_CVAR_GNI_MDD_SHARING : (Cray XC30 and Cray XC30-AC systems only.) Controls whether MPI uses dedicated or shared Memory Domain Descriptors (MDDs). If set to enabled, shared MDDs are used; if set to disabled, dedicated MDDs are used. Shared MDDs make better use of system resources. The shared MDD feature is first available on Cray XC30 and Cray XC30-AC systems running CLE 5.2-UP00 or later. This environment variable is ignored on earlier versions of CLE. Default: enabled
20220704 012151.506 INFO             PET5 index=  69                               MPIR_CVAR_GNI_MEM_DEBUG_FNAME : If set, the MPI library creates new files that correspond to the MPI processes that are about to fail due to hugepage errors and writes important memory-related statistics into these files. This information can be useful for post- processing. These files are created only for those processes (MPI ranks) that are experiencing hugepage errors. To enable this feature, set the MPICH_GNI_MEM_DEBUG_FNAME to any suitable string. The resulting files are named string.pid.MPI-rank. For example, if MPICH_GNI_MEM_DEBUG_FNAME is set to MEM_DBG_MSGS and the job fails due to hugepage errors, the resulting files will be named MEM_DBG_MSGS.pid.MPI-rank and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220704 012151.506 INFO             PET5 index=  70                              MPIR_CVAR_GNI_MAX_PENDING_GETS : Sets the maximum number of outstanding GETs a process will issue, prior to switching over to PUTs. This is typically set  dynamically based on the memory registration resources  (MPICH_GNI_NDREG_ENTRIES) allocated for each node in the job. When setting this env variable to a value, note the upper bound is dependent on MPICH_GNI_NDREG_ENTRIES.  Due to this, the final calculated value may be different than what the user requested. A setting of 0 specifies the number of pending GETs should  be 1/3 the total memory registration resources allocated for  the node. Default: -1
20220704 012151.506 INFO             PET5 index=  71                                   MPIR_CVAR_GNI_GET_MAXSIZE : Adjusts the threshold for switching between using an RDMA GET-based protocol and an RDMA PUT-based protocol for internode large message transfers. Messages qualifying for RDMA transfer that are smaller than the size specified in this environment variable use the RDMA GET-based protocol, providing buffer alignment restrictions are met. Valid values are between 16384 and 16MB, in increments of 1024. Note this value must be <= NDREG_MAXSIZE Default: on Gemini systems, 512KB; on Aries systems, 4MB. Default: -1
20220704 012151.506 INFO             PET5 index=  72                                 MPIR_CVAR_GNI_NDREG_ENTRIES : Controls the maximum number of memory registrations (per rank) allowed. Users normally should not set this environment variable, as the default is dynamic and depends on the number of ranks on the node, whether or not the application is using other software such as SHMEM or CAF, and whether or not ALPS has chosen to restrict the resources of the application for other system-wide resource limits reasons. Default: not set
20220704 012151.507 INFO             PET5 index=  73                                 MPIR_CVAR_GNI_NDREG_LAZYMEM : Controls whether or not memory deregistration uses a lazy protocol. If set to enabled, lazy memory deregistration is used; if set to disabled, lazy memory deregistration is not used, which can lead to a significant drop in bandwidth for large messages. Default: enabled
20220704 012151.507 INFO             PET5 index=  74                                 MPIR_CVAR_GNI_NDREG_MAXSIZE : Sets the maximum chunk transfer size for either a GET or a PUT.  Larger transfers are broken up into smaller chunks of MPIR_CVAR_GNI_NDREG_MAXSIZE bytes.  Note MPIR_CVAR_GNI_GET_MAXSIZE must be <= NDREG_MAXSIZE. Valid values are between 16384 and 16MB, in increments  of 1024. Default: on Gemini systems, 512KB; on Aries systems, 16MB.
20220704 012151.507 INFO             PET5 index=  75                                      MPIR_CVAR_GNI_NUM_BUFS : Controls the number of 32K byte internal buffers used by MPICH for handling eager messages. Default: 64
20220704 012151.507 INFO             PET5 index=  76                                    MPIR_CVAR_GNI_NUM_MBOXES : Sets the maximum number of mailboxes that can be allocated by MPICH. Users normally should not set this environment variable. Default: -1 (unlimited)
20220704 012151.507 INFO             PET5 index=  77                                MPIR_CVAR_GNI_RDMA_THRESHOLD : Adjusts the threshold for switching to use of the DMA engine for transferring inter-node MPI message data. The value is specified in bytes. The maximum value is 65536 and the step size is 128. Default: 1024 bytes
20220704 012151.507 INFO             PET5 index=  78                                  MPIR_CVAR_GNI_RECV_CQ_SIZE : Adjusts the GNI receive CQ size. Valid values are between 1024 and 1048576, in increments of 1024. Default: 40960
20220704 012151.507 INFO             PET5 index=  79                                  MPIR_CVAR_GNI_ROUTING_MODE : (Aries systems only.) This environment variable controls the routing mode used for off-node MPI message transfers, except when using the uGNI-optimized MPI_Alltoall and MPI_Alltoallv algorithms. The MPI_Alltoall and MPI_Alltoallv routing modes are controlled separately via the MPICH_GNI_A2A_ROUTING_MODE environment variable. The following string values are accepted; the names are not case-sensitive. IN_ORDER NMIN_HASH MIN_HASH ADAPTIVE_0 ADAPTIVE_1 ADAPTIVE_2 ADAPTIVE_3 Default: ADAPTIVE_0
20220704 012151.507 INFO             PET5 index=  80                           MPIR_CVAR_GNI_USE_UNASSIGNED_CPUS : Set this environment variable to enabled to allow MPICH to make use of unused hyperthread resources for progress threads. For more information, see MPICH_NEMESIS_ASYNC_PROGRESS. Default: enabled
20220704 012151.507 INFO             PET5 index=  81                               MPIR_CVAR_GNI_VC_MSG_PROTOCOL : This environment variable controls the protocol used for sending small messages including MPI internal control messages. The valid values are: MBOX      Use private mailboxes for receiving small messages. This approach gives the best performance in terms of message latency and message rate. MSGQ      Use shared mailboxes for receiving small messages. This approach can use significantly less memory than the MBOX protocol, although the message latency is significantly higher and the message rate is significantly lower than that obtained using the MBOX protocol. Default: MBOX
20220704 012151.507 INFO             PET5 index=  82                            MPIR_CVAR_NEMESIS_ASYNC_PROGRESS : If set, enables the MPICH asynchronous progress feature. In addition, the MPICH_MAX_THREAD_SAFETY environment variable must be set to multiple in order to enable this feature. Note:  This feature offers improved communication/computation overlap behavior for MPI-3 non- blocking collectives on systems running CLE release 5.2 UP02 or later. While this feature is backwards-compatible with earlier versions of CLE, sites interested in using this feature to obtain best performance are encouraged to upgrade to the latest version of CLE. For both Gemini and Aries systems, if MPICH_NEMESIS_ASYNC_PROGRESS is set to SC, the network interface DMA engine will enable the asynchronous progress feature. For Aries systems running CLE 5.0 or later only, if MPICH_NEMESIS_ASYNC_PROGRESS is set to MC, the Aries network interface DMA engine will employ a method that makes more efficient use of all the available virtual channels for asynchronous progress. Note:  Enabling these modes may slow down applications that lack sufficient
20220704 012151.507 INFO             PET5 index=  83                         MPIR_CVAR_NEMESIS_ON_NODE_ASYNC_OPT : If set to 1, enables the on-node MPICH asynchronous progress feature. This feature works on top of the MPICH_NEMESIS_ASYNC_PROGRESS feature to offer improved communication/computation overlap. This feature is particularly helpful in offering overlap  when the communication pattern involves large on-node transfers interleaved with off-node transfers. The on-node async-progres optimization is  meaningful only if MPICH_NEMESIS_ASYNC_PROGRESS is enabled.  Depending on the communication pattern, enabling the on-node async-progress optimization can negatively impact the the communication latency. In such cases, if the benefits of on-node async-progress are out-weighed by the higher communication latency, it is advisable to disable the on-node async-progress feature using the MPICH_NEMESIS_ON_NODE_ASYNC_OPT variable. On Cray XC systems, this feature is enabled by default. On Cray XE and XK systems, this feature is intentionally disabled by default. Users can set the MPICH_NEMESIS_ON_NODE_ASYNC_OPT to override the d
20220704 012151.507 INFO             PET5 index=  84                           MPIR_CVAR_GNI_NUM_DPM_CONNECTIONS : Determines the number of MPI-2 dynamic connections that can be established with other MPI jobs. This value is set to 0 if the MPI library is not built with MPI-2 dynamic process management enabled. The minimum number of connections is 0 and the maximum is 1048576. Default: 128
20220704 012151.507 INFO             PET5 index=  85                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220704 012151.507 INFO             PET5 index=  86                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220704 012151.507 INFO             PET5 index=  87                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220704 012151.507 INFO             PET5 index=  88                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220704 012151.507 INFO             PET5 index=  89                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through  the MPI Tools Interface, will be written by rank 0 to stderr.  If set to 2, all ranks will produce an individualized statistics  summary and write to file on a per-rank basis. The MPICH_STATS_FILE  determines the prefix of the file to be used. This information may  provide insight into how MPI performance may be improved.  Default: 0
20220704 012151.507 INFO             PET5 index=  90                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This  information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220704 012151.507 INFO             PET5 index=  91                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written  when MPICH_STATS_DISPLAY is set to 2. The filename  prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220704 012151.507 INFO             PET5 index=  92                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220704 012151.507 INFO             PET5 index=  93                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220704 012151.507 INFO             PET5 index=  94                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220704 012151.507 INFO             PET5 index=  95                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220704 012151.507 INFO             PET5 index=  96                                MPIR_CVAR_DMAPP_APP_IS_WORLD : If set, use MPMD for MPI, but treat each DMAPP application as if it is a distinct job. MPI ranks are globally contiguous and global MPI communication is possible. For each DMAPP application in the MPMD job, DMAPP ranks begin with 0 and are contiguous. DMAPP communication between applications is not possible. Note:  This feature is available only for DMAPP 7.0.1 or higher. Default: 0
20220704 012151.507 INFO             PET5 index=  97                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220704 012151.507 INFO             PET5 index=  98                                 MPIR_CVAR_MAX_THREAD_SAFETY : Specifies the maximum allowable thread-safety level that is returned by MPI_Init_thread() in the provided argument. This allows the user to control the maximum level of threading allowed. The legal values are: -------------------------------------------------------------- Value            MPI_Init_thread() returns -------------------------------------------------------------- single           MPI_THREAD_SINGLE funneled         MPI_THREAD_FUNNELED serialized       MPI_THREAD_SERIALIZED multiple         MPI_THREAD_MULTIPLE -------------------------------------------------------------- Default: MPI_THREAD_SERIALIZED Note:  Please note that the MPI_THREAD_MULTIPLE thread safety implementation is not a high-performance implementation, and that specifying MPI_THREAD_MULTIPLE can be expected to produce performance degradation as multiple thread safety uses a global lock.
20220704 012151.507 INFO             PET5 index=  99                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220704 012151.507 INFO             PET5 index= 100                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220704 012151.507 INFO             PET5 index= 101                                       MPIR_CVAR_DYNAMIC_VCS : If dynamic VCs are enabled, MPICH will only allocate the  channel-specific portion of the VC struct once communication  between the ranks is attempted.  If dynamic VCs are not  enabled, MPICH statically allocates a VC for every rank in  the job at MPI_Init time, regardless if those ranks will  communicate or not.   Default: enabled
20220704 012151.507 INFO             PET5 index= 102                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations.  On systems that do not offer High Bandwidth Memory capabilities,  (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL  (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and  MPI_Win_allocate() to be bound to either DDR, or the MCDRAM.  Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env.  variables.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET5 index= 103                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity  of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops.  On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set.  Default: SYS_DEFAULT
20220704 012151.507 INFO             PET5 index= 104                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized  memory hardware. By default, the memory policy is set to "{P}referred".  Other accepted values are "{M}andatory" and "{I}"nterleave.  Default: Preferred
20220704 012151.507 INFO             PET5 index= 105                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G.  Default: 4096
20220704 012151.507 INFO             PET5 index= 106                              MPIR_CVAR_CRAY_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized  synchronization implementation is used. If set to 0, Cray MPICH  falls back to using a pthread_mutex-based thread-synchronization  implementation. This env. variable is relevant only if the  MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE.  NOTE: This env. variable is being deprecated. Please use the  MPICH_OPT_THREAD_SYNC variable to set the thread synchronization implementation.  Default: 1
20220704 012151.507 INFO             PET5 index= 107                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. This env. variable is relevant only if the MPICH_MAX_THREAD_SAFETY variable is set to MULTIPLE. Default: 1
20220704 012151.507 INFO             PET5 index= 108                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire  a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0.  Default: 10000
20220704 012151.508 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220704 012151.508 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220704 012151.508 INFO             PET5 main: vm located at: 0x816680
20220704 012151.508 INFO             PET5 main: petCount=6 localPet=5 mypthid=140736414213184 currentSsiPe=20
20220704 012151.508 INFO             PET5 main: Current system level affinity pinning for local PET:
20220704 012151.508 INFO             PET5 main:  SSIPE=20
20220704 012151.508 INFO             PET5 main:  SSIPE=56
20220704 012151.508 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 2
20220704 012151.508 INFO             PET5 main: ssiCount=1 localSsi=0
20220704 012151.508 INFO             PET5 main: mpionly=1 threadsflag=0
20220704 012151.508 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.508 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220704 012151.508 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220704 012151.508 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220704 012151.508 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220704 012151.508 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220704 012151.508 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.508 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220704 012151.508 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220704 012151.509 INFO             PET5 Executing 'userm1_setvm'
20220704 012151.509 INFO             PET5 Executing 'userm1_register'
20220704 012151.509 INFO             PET5 Executing 'userm2_setvm'
20220704 012151.509 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.510 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220704 012151.512 INFO             PET5 Entering 'user1_run'
20220704 012151.512 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220704 012151.513 INFO             PET5 model1: vm located at: 0x894370
20220704 012151.513 INFO             PET5 model1: petCount=6 localPet=5 mypthid=140736414213184 currentSsiPe=5
20220704 012151.513 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220704 012151.513 INFO             PET5 model1:  SSIPE=5
20220704 012151.513 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012151.513 INFO             PET5 model1: ssiCount=1 localSsi=0
20220704 012151.513 INFO             PET5 model1: mpionly=1 threadsflag=0
20220704 012151.513 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012151.513 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220704 012151.513 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220704 012151.513 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220704 012151.513 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220704 012151.513 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220704 012151.513 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012151.513 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220704 012151.513 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220704 012151.513 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012152.626 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012153.647 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012154.666 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012155.685 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012156.703 INFO             PET5 Exiting 'user1_run'
20220704 012201.425 INFO             PET5 Entering 'user1_run'
20220704 012201.425 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220704 012201.425 INFO             PET5 model1: vm located at: 0x894370
20220704 012201.425 INFO             PET5 model1: petCount=6 localPet=5 mypthid=140736414213184 currentSsiPe=5
20220704 012201.425 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220704 012201.425 INFO             PET5 model1:  SSIPE=5
20220704 012201.425 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220704 012201.425 INFO             PET5 model1: ssiCount=1 localSsi=0
20220704 012201.425 INFO             PET5 model1: mpionly=1 threadsflag=0
20220704 012201.425 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220704 012201.425 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220704 012201.425 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220704 012201.425 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220704 012201.425 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220704 012201.425 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220704 012201.425 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220704 012201.425 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220704 012201.425 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220704 012201.425 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012202.454 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012203.477 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012204.493 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012205.512 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220704 012206.530 INFO             PET5 Exiting 'user1_run'
20220704 012211.191 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220704 012211.191 INFO             PET5  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220704 012211.191 INFO             PET5 Finalizing ESMF
